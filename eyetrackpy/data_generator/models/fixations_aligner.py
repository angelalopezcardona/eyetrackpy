from transformers import (
    BatchEncoding,
)
from tokenizeraligner.models.tokenizer_aligner import TokenizerAligner


class FixationsAligner:
    @staticmethod
    def map_features_between_tokens_info(
        features: list,
        tokens_id_mapped: list,
        input_ids_original,
        text_tokenized_model,
        text_tokenized_fix,
        words_str_mapped,
        tokens_str_mapped,
    ):
        features_pair, features_pair_all = [], []
        features_mapped = []
        for i in range(len(tokens_id_mapped)):
            features_idx, features_mapped_idx = 0, 0
            features_pair.append([])
            features_pair_all.append([])
            features_mapped.append([])
            for j in range(len(tokens_id_mapped[i])):
                pair = tokens_id_mapped[i][j]
                # compute pair features for the pair
                k, fix_pair = 0, 0
                while k < len(pair[1]):
                    if text_tokenized_fix["input_ids"][i][features_idx] == pair[1][k]:
                        fix_pair += features[i][features_idx]
                        k += 1
                    features_idx += 1

                # asign features to tke model tokens in the pair
                features_mapped_idx_sec = features_mapped_idx
                k, fix_mapped_pair_list = 0, []
                while k < len(pair[0]):
                    if (
                        text_tokenized_model["input_ids"][i][features_mapped_idx]
                        == pair[0][k]
                    ):
                        fix_mapped_pair_list.append(features_mapped_idx)
                        k += 1
                    features_mapped_idx += 1
                fix_value = fix_pair / len(fix_mapped_pair_list)

                while len(features_mapped[i]) < features_mapped_idx:
                    if (
                        text_tokenized_model["input_ids"][i][len(features_mapped[i])]
                        in pair[0]
                    ):
                        features_mapped[i].append(fix_value)
                    else:
                        features_mapped[i].append(0)
                # ------------------------------------------------
                # compute pair features for the pair
                k, fix_pair_sec = 0, 0
                while k < len(pair[0]):
                    if (
                        text_tokenized_model["input_ids"][i][features_mapped_idx_sec]
                        == pair[0][k]
                    ):
                        fix_pair_sec += features_mapped[i][features_mapped_idx_sec]
                        k += 1
                    features_mapped_idx_sec += 1
                # ------------------------------------------------
                features_pair[i].append(fix_pair)
                features_pair_all[i].append(
                    [
                        (fix_pair),
                        (fix_pair_sec),
                        words_str_mapped[i][j],
                        tokens_str_mapped[i][j],
                    ]
                )

        return features_pair, features_pair_all, features_mapped

    @staticmethod
    def map_features_lcs(
        features: list,
        input_ids_original,
        text_tokenized_model,
    ):
        features_corrected = []
        # ----------------<> start <>  correct the dimension to the original dimension----------------
        # input_ids_original is the original tokens. It may contain special tokens that where remove to compute the features
        # text_tokenized_modelis the tokens generated by the model tokenizer.
        # we compute common_tokens, the tokens that are in both input_ids_original and text_tokenized_model["input_ids"]
        # we need to map features to features_corrected, so in all the no common tokens that are not  in input_ids_original we add ceros.
        comtok_idx = 0
        features_corrected = []
        m = 0
        common_tokens = TokenizerAligner().longest_common_subsequence(
            input_ids_original, text_tokenized_model
        )
        for l in range(len(input_ids_original)):
            # if we common tokens are finished, we add ceros as features until padding
            # we add ceros as features until padding
            if comtok_idx >= len(common_tokens):
                features_corrected.extend(
                    [0] * (len(input_ids_original) - len(features_corrected))
                )
                break
            # we search in original tokens the next common token, if not, me move one and append 0
            if input_ids_original[l] != common_tokens[comtok_idx]:
                features_corrected.append(0)
                continue
            # we search in the new tokens the next common token index
            while text_tokenized_model[m] != common_tokens[comtok_idx]:
                m += 1

            if m >= len(features):
                # we add ceros as features until padding
                features_corrected.extend(
                    [0] * (len(input_ids_original) - len(features_corrected))
                )
                break

            # we append the features to the common token
            features_corrected.append(features[m])
            comtok_idx += 1
            m += 1
        # ----------------</> finish </> correct the dimension to the original dimension----------------
        return features_corrected

    @staticmethod
    def map_fixations_between_tokens_correct(
        features: list,
        tokens_id_mapped: list,
        input_ids_original,
        text_tokenized_model: BatchEncoding,
        text_tokenized_fix: BatchEncoding,
        return_all=False,
    ):
        features_mapped, features_mapped_corrected = [], []
        # check if tokens_id_mapped is a list of lists
        if not isinstance(features[0], list):
            features = [features]

        for i in range(len(tokens_id_mapped)):
            if tokens_id_mapped[i] is None:
                # if alignent failed I just asign the features to the original tokens, adding zeros until dimension
                features_mapped_corrected_i = TokenizerAligner().adjust_list_length(
                    features[i], input_ids_original[i]
                )
                features_mapped_corrected.append(features_mapped_corrected_i)
            else:
                # ----------------<> start <>  pass features to the remapped tokens ----------------
                features_mapped_i = FixationsAligner().map_features_between_tokens(
                    features[i],
                    tokens_id_mapped[i],
                    text_tokenized_model["input_ids"][i],
                    text_tokenized_fix["input_ids"][i],
                )
                features_mapped.append(features_mapped_i)
                # ----------------<> finish <>  pass features to the remapped tokens ----------------
                features_mapped_corrected_i = FixationsAligner().map_features_lcs(
                    features_mapped[i],
                    input_ids_original[i],
                    text_tokenized_model["input_ids"][i],
                )
                features_mapped_corrected.append(features_mapped_corrected_i)
        if len(features_mapped_corrected[i]) != len(input_ids_original[i]):
            print(len(features_mapped_corrected[i]), "-", len(input_ids_original[i]))
            print("error")
        if return_all:
            return features_mapped_corrected, features_mapped

        return features_mapped_corrected

    @staticmethod
    def map_features_between_tokens_verbose(
        features: list,
        features_mapped: list,
        tokens_id_mapped: list,
        text_tokenized_model: list,
        text_tokenized_fix: list,
    ):
        tokens_pair = []
        features_pair = []
        features_idx, features_mapped_idx = 0, 0
        for j in range(len(tokens_id_mapped)):
            features_pair_fix, features_pair_model = [], []
            tokens_pair_fix, tokens_pair_model = [], []
            pair = tokens_id_mapped[j]
            k = 0
            while k < len(pair[1]):
                if text_tokenized_fix[features_idx] == pair[1][k]:
                    features_pair_fix.append(features[features_idx])
                    tokens_pair_fix.append(text_tokenized_fix[features_idx])
                    k += 1
                features_idx += 1
            k, features_mapped_idx = 0, 0
            while k < len(pair[0]):
                if text_tokenized_model[features_mapped_idx] == pair[0][k]:
                    features_pair_model.append(features_mapped[features_mapped_idx])
                    tokens_pair_model.append(text_tokenized_model[features_mapped_idx])
                    k += 1
                features_mapped_idx += 1
            features_pair.append((features_pair_model, features_pair_fix))
            tokens_pair.append((tokens_pair_model, tokens_pair_fix))
        return features_pair, tokens_pair

    @staticmethod
    def map_features_between_tokens(
        features: list,
        tokens_id_mapped: list,
        text_tokenized_model: list,
        text_tokenized_fix: list,
        mode="mean",
    ):
        features_idx, features_mapped_idx = 0, 0
        features_mapped = []
        for j in range(len(tokens_id_mapped)):
            pair = tokens_id_mapped[j]
            # compute pair features for the pair of tokens of the features model
            k, features_pair = 0, []
            while k < len(pair[1]):
                if text_tokenized_fix[features_idx] == pair[1][k]:
                    features_pair.append(features[features_idx])
                    k += 1
                features_idx += 1

            # asign features to the tokens of the new model in the pair
            k, fix_mapped_pair_list = 0, []
            while k < len(pair[0]):
                if text_tokenized_model[features_mapped_idx] == pair[0][k]:
                    fix_mapped_pair_list.append(features_mapped_idx)
                    k += 1
                features_mapped_idx += 1
            if mode == "mean":
                feature_value = sum(features_pair) / len(fix_mapped_pair_list)
            elif mode == "max":
                feature_value = max(features_pair)
            elif mode == "sum":
                feature_value = sum(features_pair)
            # feature_value = features_pair / len(fix_mapped_pair_list)
            # we assign the value to the model tokens is they are in the paired mapped ones, if not we add a cero
            while len(features_mapped) < features_mapped_idx:
                if text_tokenized_model[len(features_mapped)] in pair[0]:
                    features_mapped.append(feature_value)
                else:
                    features_mapped.append(0)
        # here we dont add ceros until we reach the mokel tokens dimension, because we are going to correct it later to the original
        # ----------------<> finish <>  pass features to the remapped tokens ----------------
        return features_mapped

    @staticmethod
    def _map_features_from_tokens_to_words(
        features: list,
        word_token_idx: list,
        mode="max",
    ):
        features_mapped = []
        for j in range(1 + max([x if x is not None else 0 for x in word_token_idx])):
            idx = [i for i, x in enumerate(word_token_idx) if x == j]
            if len(idx) == 0:
                features_mapped.append(0)
            else:
                features_list = [features[i] for i in idx]
                if mode == "max":
                    features_mapped.append(max(features_list))
                elif mode == "sum":
                    features_mapped.append(sum(features_list))
                elif mode == "mean":
                    features_mapped.append(sum(features_list) / len(idx))
                else:
                    raise ValueError("mode must be max, sum or mean")
        return features_mapped

    @staticmethod
    def map_features_from_tokens_to_words(
        features: list,
        text_tokenized: BatchEncoding,
        mode="max",
    ):
        word_token_idx = text_tokenized.word_ids()
        if sum([x for x in word_token_idx if x is not None ]) == 0:
            return None
        return FixationsAligner()._map_features_from_tokens_to_words(
            features, word_token_idx, mode=mode
        )

    @staticmethod
    def map_features_from_tokens_to_words_batch(
        features: list,
        text_tokenized: BatchEncoding,
        mode="max",
    ):
        features_mapped = []
        word_token_idx = [
            text_tokenized[i].word_ids for i in range(len(text_tokenized["input_ids"]))
        ]
        for i in range(len(word_token_idx)):
            features_mapped.append(
                FixationsAligner()._map_features_from_tokens_to_words(
                    features[i], word_token_idx[i], mode=mode
                )
            )
        return features_mapped

    @staticmethod
    def map_features_from_words_to_words(
        list_words_first: list,
        text: str,
        text_tokenized: BatchEncoding,
        features: list,
        mode="max",
    ):
        # we compute the words list from the text_tokenized
        list_words_second = TokenizerAligner().text_to_words(
            text, text_tokenized, text_tokenized.word_ids()
        )
        # we map the original words (list_word_first) to the words in the text_tokenized
        mapped_words_idxs, mapped_words_str = TokenizerAligner().map_words(
            list_words_first, list_words_second
        )
        # we map the features from the first list to the second list
        # we compute the the words features from the tokens features
        features_mapped = []
        for features_tokens in features:
            features_mapped_second_words = (
                FixationsAligner().map_features_from_tokens_to_words(
                    features_tokens, text_tokenized, mode="max"
                )
            )
            # if features_mapped_second_words is None:
            #     tokens_str = TokenizerAligner().tokens_to_words(text, text_tokenized)
            features_mapped_first_words = (
                TokenizerAligner().map_features_between_paired_list(
                    features_mapped_second_words,
                    mapped_words_idxs,
                    list_words_first,
                    mode="max",
                )
            )
            features_mapped.append(features_mapped_first_words)

        return features_mapped
